Q.1 Can you provide an example of how you would use Pandas to clean and preprocess a large dataset for analysis in Amazon's data ecosystem?
1 Clean nulls and duplicates
2 Standardize column formats
3 Feature engineer fields like purchase_month, basket_size
4 Save the processed dataset for analysis or loading into Redshift

Q.2 How would you handle missing data in a dataset using Python, and why is it important in the context of Business Intelligence?
WHY 
Accuracy: Dashboards and reports often drive executive decisions — missing or incorrect data affects KPIs (e.g., sales, conversion rates, retention).
Trust: If stakeholders see blanks, inconsistencies, or frequent "N/A", they may lose confidence in your analytics.
Model Performance: If you're feeding data into machine learning or forecasting models, missing values can degrade performance or cause failure.

HOW 
| Strategy                   | Use When…                                 | Example Code                                                       |
| -------------------------- | ----------------------------------------- | ------------------------------------------------------------------ |
| Drop Rows                  | Data point is incomplete or outlier-heavy | `df.dropna()`                                                      |
| Fill with Default/Constant | Business logic permits a safe default     | `df.fillna(0)` or `df['region'].fillna('Unknown')`                 |
| Fill with Mean/Median      | For continuous variables (e.g., sales)    | `df['discount'].fillna(df['discount'].mean())`                     |
| Forward/Backward Fill      | Time series or sequential data            | `df.fillna(method='ffill')`                                        |
| Impute from Business Logic | Use domain-specific logic                 | e.g., If `payment_type` missing for `COD` orders, fill as `'Cash'` |

Q.3 Describe a situation where you had to optimize Python code for performance. What techniques did you use, and how would you apply them to Amazon's BI task? 
SITUATION:
While working on a customer segmentation project for a retail client, I had a Python script that processed 8M+ transaction records. The original script took over 30 minutes to execute, 
which slowed down daily reporting and stakeholder insights.

TASK:
I needed to optimize the code to reduce execution time without compromising accuracy, so that the marketing team could run daily cohort reports in near real-time.

ACTION: 
1. Vectorization using Pandas:
I replaced loops and .apply() functions with native groupby and agg operations.
2. Chunking large CSV files with Dask or pd.read_csv(chunksize=...):
Instead of loading everything into memory, I processed data in chunks and aggregated later.
3. Caching intermediate results:
Used joblib to cache expensive calculations (e.g., customer-level aggregations) so they wouldn't re-run unnecessarily.
4. Parallel processing:
Used concurrent.futures.ThreadPoolExecutor to process customer segments in parallel batches.

RESULT:
Reduced script runtime from 30+ minutes to under 4 minutes.
Improved reliability and enabled the BI team to automatically refresh dashboards before the daily stand-up.

Q.4 Create a Python function that emulates the behavior of an SQL INNER JOIN between two lists of dictionaries. The lists represent tables, and the dictionaries represent rows.
Q.5 Create a Python class that maintains a rolling average of the last N numbers added to it. The class should have methods to add a new number and retrieve the current rolling average.
